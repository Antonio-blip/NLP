{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install rarfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8PesbY4qGLB",
        "outputId": "0f7e5ccb-07e1-4398-ed26-182babc8e49a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYH-eFzcEqn2",
        "outputId": "ce280952-cfbe-4d94-b9dc-ae5c1fe4b859"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import rarfile\n",
        "from IPython.display import Markdown, display\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "\n",
        "#Recursos NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ4UxSDTOzNR",
        "outputId": "05899300-5aef-4ec9-aa85-96955588ccca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Especifica el nombre del archivo RAR\n",
        "rar_file_name = 'data.rar'  # Reemplaza con el nombre correcto\n",
        "\n",
        "try:\n",
        "    # Intenta extraer los archivos del RAR en el directorio actual\n",
        "    rar = rarfile.RarFile(rar_file_name)\n",
        "    rar.extractall()\n",
        "\n",
        "    # Elimina el archivo RAR después de la extracción\n",
        "    os.remove(rar_file_name)\n",
        "\n",
        "    print(f'El archivo {rar_file_name} ha sido eliminado con éxito y se ha extraído el contenido en la carpeta /data.')\n",
        "except FileNotFoundError:\n",
        "    # Si el archivo RAR no se encuentra, muestra un mensaje personalizado\n",
        "    display(Markdown(f'<font color=\"red\"><b>El archivo {rar_file_name} no se encuentra en el directorio actual o no puede ser eliminado.</b></font>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgzCwTXQK25u",
        "outputId": "8bb4f5f9-ca53-456a-aab8-ab5e9384817f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo data.rar ha sido eliminado con éxito y se ha extraído el contenido en la carpeta /data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Especifica la ruta del archivo CSV\n",
        "csv_file_path = 'data/corpusMini_df.csv'\n",
        "# Carga el archivo CSV en un DataFrame\n",
        "corpusMini_df = pd.read_csv(csv_file_path)\n",
        "# Muestra las primeras 5 filas del DataFrame\n",
        "print(corpusMini_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYg0cq8OMpAa",
        "outputId": "663252f4-b3f9-4240-a404-ce8af7792128"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       reviewerID        asin             reviewerName  helpful  \\\n",
            "0   AN4KLPNB56X3Z  B000MD3MIW                   carrie   [3, 3]   \n",
            "1   A60I915C5M3JE  B000J3HZWE  Ellen Dawson \"seriousb\"   [2, 3]   \n",
            "2  A15Q9YEG1XPEJN  B00I18UVO8         Get What We Give   [3, 6]   \n",
            "3  A3V9TR2U1KISVK  B0029NGZ5K                Sarah1989  [6, 13]   \n",
            "4  A3QD59N3M7O7KB  B0015GIPDW                  Xina143   [4, 5]   \n",
            "\n",
            "                                          reviewText  overall  \\\n",
            "0  I was replacing another petsafe door with this...      1.0   \n",
            "1  I read all of the reviews before purchasing an...      1.0   \n",
            "2  I didn't realize when I ordered this product t...      1.0   \n",
            "3  There is very little meat in this food and the...      1.0   \n",
            "4  We used this skimmer for a few months, but fou...      1.0   \n",
            "\n",
            "                                             summary  unixReviewTime  \\\n",
            "0                                      Horrible Door      1340323200   \n",
            "1                                     Waste of Money      1283472000   \n",
            "2  I cannot in good conscience give this treat to...      1396569600   \n",
            "3                             Very poor quality food      1274227200   \n",
            "4                                 Disposable skimmer      1238025600   \n",
            "\n",
            "    reviewTime  sentiment  \n",
            "0  06 22, 2012          0  \n",
            "1   09 3, 2010          0  \n",
            "2   04 4, 2014          0  \n",
            "3  05 19, 2010          0  \n",
            "4  03 26, 2009          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la lista de stopwords en inglés de NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Inicializar el lematizador de palabras\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenizamos, lematizamos, quitamos stopwords y eliminamos caracteres especiales.\n",
        "def preprocess_text(text):\n",
        "    # Eliminar caracteres especiales excepto letras, espacios, \"/\", \".\", \"--\" y puntos seguidos de dígitos\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\.', ' ', text) #Se han detectado comentarios con valores del estilo chemical.Various o meat.Caramel, por tanto se substituyen puntos por espacios.\n",
        "    text = re.sub(r'[^a-zA-Z\\s/.\\-]|(?<!\\d)\\.(?!\\d)|http\\S+', '', text) #Se eliminan carácteres extraños antes de lematizar.\n",
        "    words = re.split(r'[./\\-\\s]+|--+', text) # Separar por \"/\", \".\", \"-\" y espacios\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
        "    cleaned_text = ' '.join(lemmatized_words)\n",
        "    # Eliminar stopwords\n",
        "    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Eliminar espacios en blanco adicionales\n",
        "    return cleaned_text.strip()  # Eliminar espacios en blanco al principio y al final\n",
        "\n",
        "# Aplicar la función de preprocesamiento a la columna 'reviewText'\n",
        "corpusMini_df['preprocessed_text'] = corpusMini_df['reviewText'].apply(preprocess_text)\n",
        "\n",
        "# Tokenizar el texto preprocesado en palabras individuales\n",
        "tokenized_words = [word for text in corpusMini_df['preprocessed_text'] for word in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "3NseQL79yDWP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir algunas filas del DataFrame con el texto preprocesado\n",
        "for index, row in corpusMini_df.head(6).iterrows():\n",
        "    print(f\"Original Text: {row['reviewText']}\")\n",
        "    print(f\"Preprocessed Text: {row['preprocessed_text']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "sQ_Zix-EIaVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ee2b0b-4372-4cf3-a26d-f6034beb909a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I was replacing another petsafe door with this one.  The measurements were the same.  To say the least it did not fit.  I had already cut the inside wall, as directed, and it still didn't fit.  Can't return it since I had technically altered the door.  Very flimy and cheap. Finally put the old door back in and drilled holes in the flap of the new to make it fit my old door. The rest is garbage.  The old doors were much stronger and easier to install.  I would never recommend one of these new models.  It would never last, regardless of the weather.  I would give this a negative number if available.  Don't waste your money.\n",
            "Preprocessed Text: replace another petsafe door one measurements say least fit already cut inside wall direct still didnt fit cant return since technically alter door flimy cheap finally put old door back drill hole flap new make fit old door rest garbage old doors much stronger easier install would never recommend one new model would never last regardless weather would give negative number available dont waste money\n",
            "\n",
            "Original Text: I read all of the reviews before purchasing and was extremely hopeful because my black lab/husky/who's your daddy rescue dog is afraid of thunderstorms, gunfire, fireworks, etc. I have tried Rescue Remedy, soothing music, Thundershirt, Mutt Muffs and now the Farnam Comfort Zone DAP with no relief in site. He still tries to hide behind the tv or under the computer desk...anywhere there are wires to get tangled in it seems. It's not only his extreme distress but also the potential damage that his frantic state could cause and I hate having to put him in his crate every time. I am becoming frantic too trying to help him but the DAP isn't it either.....extremely disappointed!\n",
            "Preprocessed Text: read review purchase extremely hopeful black lab husky whos daddy rescue dog afraid thunderstorms gunfire fireworks etc try rescue remedy soothe music thundershirt mutt muff farnam comfort zone dap relief site still try hide behind tv computer desk anywhere wire get tangle seem extreme distress also potential damage frantic state could cause hate put crate every time become frantic try help dap isnt either extremely disappoint\n",
            "\n",
            "Original Text: I didn't realize when I ordered this product that it was the same product that had been recalled last year due to multiple dog deaths and illnesses. I thought that product was gone for good.Then I received the Waggin Train Smoky Jerky Snacks and was pleased to see &#34;made in the USA&#34; emblazoned on the back of the bag.  However, I went Googling and came across an article that stated the following:&#34;Waggin&#8217; Train now contracts with a single-source, U.S.-based supplier for raw materials at a newly built facility in China and has its own in-house inspectors on site, according to Nestl&eacute; Purina&#8217;s vice president of manufacturing Bill Cooper, who oversaw the changes in China. The supplier, which Nestl&eacute; Purina declined to name, is one that Waggin&#8217; Train has contracted with previously.&#8220;We certainly had very stringent testing prior to the withdrawal, but we&#8217;ve enhanced that,&#8221; Cooper said. &#8220;We&#8217;ve also met with the FDA on several occasions to go through our testing. We are confident the quality controls are better than anyone else&#8217;s manufacturing jerky treats.&#8221;To me, this means that the ingredients may come from the U.S. but they are still being made in China, which is NOT Made in the USA. Who knows what is going on or if the ingredients are spoiling during shipment.I will not be giving these to my beloved companion. They will go directly into the trash.\n",
            "Preprocessed Text: didnt realize order product product recall last year due multiple dog deaths illnesses think product go good receive waggin train smoky jerky snack please see make usa emblazon back bag however go google come across article state followingwaggin train contract single source u base supplier raw materials newly build facility china house inspectors site accord nestleacute purinas vice president manufacture bill cooper oversee change china supplier nestleacute purina decline name one waggin train contract previously certainly stringent test prior withdrawal weve enhance cooper say weve also meet fda several occasion go test confident quality control better anyone elses manufacture jerky treat mean ingredients may come u still make china make usa know go ingredients spoil shipment give beloved companion go directly trash\n",
            "\n",
            "Original Text: There is very little meat in this food and there are toxic chemicals.Various Meat By Products--lower quality than whole meat or meals and harder to digest than actual meat.Caramel coloring is only necessary when there is not enough meat.  Caramel color may be carcinogenic.\"Liquid Top Sirloin flavor\"  \"Grilled chicken flavor\"--  Flavoring is only necessary when the product has insufficient meat.  Top Sirloin flavoring is made from manure of cattle.Carageenan causes cancer, including breast cancer.  Read an interview with Joanne Tobacman, MD, professor of internal medicine, University of Iowa at [...]There are no vegetables in this food.Cesar has repeatedly stated that his entire goal in coming to the United States was to go to Hollywood and be famous.  I think that that focus on fame (and money) is apparent in his approach to dog food:  Give crummy food nice packaging and an appealing name.That one's dog will eat it is no advertisement.  First, the only thing one really has to do to sell dog food is make it so the dogs will eat it.  That the dog will eat it is no measure of the quality or benefit of the food.  Dogs eat anything--including manure, which is part of this food as the meat flavoring.\n",
            "Preprocessed Text: little meat food toxic chemicals various meat products lower quality whole meat meals harder digest actual meat caramel color necessary enough meat caramel color may carcinogenic liquid top sirloin flavor grill chicken flavor flavor necessary product insufficient meat top sirloin flavor make manure cattle carageenan cause cancer include breast cancer read interview joanne tobacman md professor internal medicine university iowa vegetables food cesar repeatedly state entire goal come unite state go hollywood famous think focus fame money apparent approach dog food give crummy food nice package appeal name ones dog eat advertisement first thing one really sell dog food make dog eat dog eat measure quality benefit food dog eat anything include manure part food meat flavor\n",
            "\n",
            "Original Text: We used this skimmer for a few months, but found that it removed very little waste. Instead of waste it removed a tea cup of almost-clear water every few days. The air pump adjustment did very little to make it more efficient. So we determined it was useless, and we would either live without or purchase a better unit.\n",
            "Preprocessed Text: use skimmer months find remove little waste instead waste remove tea cup almost clear water every days air pump adjustment little make efficient determine useless would either live without purchase better unit\n",
            "\n",
            "Original Text: This was not good for my purposes. Bathe material wasn't sturdy enough to control my dogs.  May be good for a smaller dog, but not for my purposes.\n",
            "Preprocessed Text: good purpose bathe material wasnt sturdy enough control dog may good smaller dog purpose\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la cantidad de tokenized words\n",
        "cantidad_tokenized_words = len(tokenized_words)\n",
        "\n",
        "# Imprimir la cantidad de tokens\n",
        "print(f\"La cantidad de tokenized words es: {cantidad_tokenized_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNDXKdzU2uDd",
        "outputId": "5a5324a6-8e6a-4257-f234-92608c423ca7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La cantidad de tokenized words es: 145374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener una lista de todas las palabras únicas en el corpus\n",
        "unique_words = set(tokenized_words)\n",
        "\n",
        "# Calcular la cardinalidad del vocabulario\n",
        "vocab_cardinality = len(unique_words)\n",
        "\n",
        "# Imprimir la cardinalidad del vocabulario (tokens únicos)\n",
        "print(f\"Cardinalidad del Vocabulario: {vocab_cardinality}\")\n"
      ],
      "metadata": {
        "id": "-TrCdmv3In1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26af0da8-5e7a-4a8d-a361-338304e3a2c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cardinalidad del Vocabulario: 9702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coger muestra del corpus\n",
        "text_data = ' '.join(corpusMini_df['preprocessed_text'])  # Concatenate preprocessed text from the DataFrame\n",
        "\n",
        "# Definir las palabras token\n",
        "words = nltk.word_tokenize(text_data)\n",
        "\n",
        "# Definir los ngrams\n",
        "N_values = [2, 3]  # You can add more values for other N-grams\n",
        "\n",
        "for N in N_values:\n",
        "    # Generar N-grams\n",
        "    n_grams = list(ngrams(words, N))\n",
        "\n",
        "    # Calcular la frecuencia de distribución de los ngrams\n",
        "    freq_dist = FreqDist(n_grams)\n",
        "\n",
        "    # Obtener los ngrams más comunes\n",
        "    most_common_ngrams = freq_dist.most_common(10)  # Change 10 to your desired number\n",
        "\n",
        "    # Imprimir el resultado\n",
        "    print(f\"Top {N}-grams:\")\n",
        "    for ngram, frequency in most_common_ngrams:\n",
        "        print(f\"{ngram}: {frequency} times\")\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "c5OgNL3iMHSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5afc10-f614-42c0-e184-0f53d2309a1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 2-grams:\n",
            "('litter', 'box'): 163 times\n",
            "('dog', 'food'): 154 times\n",
            "('dog', 'love'): 129 times\n",
            "('work', 'well'): 109 times\n",
            "('think', 'would'): 104 times\n",
            "('dont', 'know'): 95 times\n",
            "('cat', 'love'): 92 times\n",
            "('can', 'not'): 87 times\n",
            "('waste', 'money'): 84 times\n",
            "('would', 'recommend'): 84 times\n",
            "\n",
            "\n",
            "Top 3-grams:\n",
            "('dry', 'dog', 'food'): 18 times\n",
            "('would', 'recommend', 'product'): 17 times\n",
            "('dont', 'waste', 'money'): 16 times\n",
            "('buy', 'another', 'one'): 16 times\n",
            "('last', 'long', 'time'): 14 times\n",
            "('clean', 'litter', 'box'): 12 times\n",
            "('year', 'old', 'cat'): 11 times\n",
            "('think', 'would', 'try'): 11 times\n",
            "('two', 'year', 'old'): 10 times\n",
            "('would', 'recommend', 'anyone'): 10 times\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las 10 palabras más comunes para sentimiento 0\n",
        "sentiment_0_reviews = corpusMini_df[corpusMini_df['sentiment'] == 0]\n",
        "tokenized_words_sentiment_0 = [word for text in sentiment_0_reviews['preprocessed_text'] for word in word_tokenize(text)]\n",
        "freq_dist_sentiment_0 = FreqDist(tokenized_words_sentiment_0)\n",
        "common_words_sentiment_0 = freq_dist_sentiment_0.most_common(10)\n",
        "\n",
        "# Imprimir las 10 palabras más comunes para sentimiento 0\n",
        "print(\"10 palabras más comunes para sentimiento 0:\")\n",
        "for word, frequency in common_words_sentiment_0:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "# Obtener las 10 palabras más comunes para sentimiento 1\n",
        "sentiment_1_reviews = corpusMini_df[corpusMini_df['sentiment'] == 1]\n",
        "tokenized_words_sentiment_1 = [word for text in sentiment_1_reviews['preprocessed_text'] for word in word_tokenize(text)]\n",
        "freq_dist_sentiment_1 = FreqDist(tokenized_words_sentiment_1)\n",
        "common_words_sentiment_1 = freq_dist_sentiment_1.most_common(10)\n",
        "\n",
        "# Imprimir las 10 palabras más comunes para sentimiento 1\n",
        "print(\"10 palabras más comunes para sentimiento 1:\")\n",
        "for word, frequency in common_words_sentiment_1:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FwEwJd40FKNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac9b5a8-6695-4670-cc18-332dedec8937"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 palabras más comunes para sentimiento 0:\n",
            "dog: 1349\n",
            "cat: 967\n",
            "get: 905\n",
            "one: 742\n",
            "use: 721\n",
            "would: 668\n",
            "like: 668\n",
            "product: 540\n",
            "buy: 536\n",
            "make: 513\n",
            "10 palabras más comunes para sentimiento 1:\n",
            "dog: 1350\n",
            "cat: 949\n",
            "get: 845\n",
            "use: 768\n",
            "one: 739\n",
            "like: 713\n",
            "love: 575\n",
            "would: 538\n",
            "food: 508\n",
            "work: 480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar una columna 'processedReview' basada en 'preprocessed_text'\n",
        "corpusMini_df['processedReview'] = corpusMini_df['preprocessed_text']\n",
        "\n",
        "# Reemplazar valores vacíos en 'processedReview' con NaN\n",
        "corpusMini_df['processedReview'] = corpusMini_df['processedReview'].replace('', np.nan)\n",
        "\n",
        "# Eliminar filas con NaN en 'processedReview'\n",
        "corpusMini_df = corpusMini_df.dropna(subset=['processedReview'])\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "corpusMini_df.to_csv('/content/data/corpusMini_df2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "7CAO4yEcSxC_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Acceder a la columna 'processedReview'\n",
        "nueva_columna = corpusMini_df['processedReview']\n",
        "\n",
        "# Imprimir las primeras 10 filas de la nueva columna\n",
        "print(nueva_columna.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9a8pdGszQWA",
        "outputId": "9bc7fad8-b49f-48ea-9131-3fefe7066e5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    replace another petsafe door one measurements ...\n",
            "1    read review purchase extremely hopeful black l...\n",
            "2    didnt realize order product product recall las...\n",
            "3    little meat food toxic chemicals various meat ...\n",
            "4    use skimmer months find remove little waste in...\n",
            "5    good purpose bathe material wasnt sturdy enoug...\n",
            "6    purchase border collie package suggest halti s...\n",
            "7    review gph version dont know versions one litt...\n",
            "8    cat buy stop scratch furniture scratch post ho...\n",
            "9    buy think commercial air pump would better job...\n",
            "Name: processedReview, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con todo este procesado, deberiamos de tener palabras significativas y el texto completamente limpio, exceptuando alguna falta de ortografía.\n",
        "Por último aunque sospechemos que dog, cat, etc sean palabras irrelevantes las trataremos en la siguiente actividad, con TfidfVectorizer igual que las outlyers producidos por las faltas ortográficas."
      ],
      "metadata": {
        "id": "xlc_HlwIANne"
      }
    }
  ]
}